{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis de sentimientos en Twitter\n",
    "\n",
    "Este programa entrena un modelo a partir de tres conjuntos de entrenamiento de 230 casos para tweets positivos, negativos y neutros. Posteriormente, pide una query con la que consulta Twitter y obtiene 100 tweets que analiza y entrega un resultado del analisis de sentimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LALA\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import numpy as np # array processing for numbers, strings, records, and objects.\n",
    "import random # generate pseudo random numbers\n",
    "import tweepy # twitter\n",
    "import urllib3 # http client\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn import preprocessing # utility functions and transformer classes\n",
    "from sklearn.preprocessing import StandardScaler # transforma raw data\n",
    "from sklearn.linear_model import LogisticRegression # regresion logistica\n",
    "from sklearn.neighbors import KNeighborsClassifier # kneighbors\n",
    "from sklearn.svm import SVC # support vector machine\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# carga de 230 ejemplos de cada caso\n",
    "TWEETS_NEG = '/home/dgomez/comp/machine_learning/proyectos/sentiment/TWEETS_NEGATIVO'\n",
    "TWEETS_NEU = '/home/dgomez/comp/machine_learning/proyectos/sentiment/TWEETS_NEUTRO'\n",
    "TWEETS_POS = '/home/dgomez/comp/machine_learning/proyectos/sentiment/TWEETS_POSITIVO'\n",
    "\n",
    "# carga de credenciales de twitter\n",
    "file_keys = open(\"/home/dgomez/comp/machine_learning/proyectos/sentiment/TWITTER_KEYS\", \"r\") \n",
    "line = file_keys.readline(); CONSUMER_KEY        = line.split(':')[1][1:-2]\n",
    "line = file_keys.readline(); CONSUMER_SECRET     = line.split(':')[1][1:-2]\n",
    "line = file_keys.readline(); ACCESS_TOKEN        = line.split(':')[1][1:-2]\n",
    "line = file_keys.readline(); ACCESS_TOKEN_SECRET = line.split(':')[1][1:-2]\n",
    "\n",
    "NLTK_DIR='/home/dgomez/comp/machine_learning/proyectos/sentiment/sentiment_web/nltk_data'\n",
    "nltk.data.path.append(NLTK_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizar texto de los tweets\n",
    "def normalize(text):\n",
    "    text = text.lower()\n",
    "    #text = text.replace('á', 'a')\n",
    "    text = re.sub('á', 'a', text)\n",
    "    text = re.sub('é', 'e', text)\n",
    "    text = re.sub('í', 'i', text)\n",
    "    text = re.sub('ó', 'o', text)\n",
    "    text = re.sub('ú', 'u', text)\n",
    "    text = re.sub('ü', 'u', text)\n",
    "    text = re.sub('Á', 'a', text)\n",
    "    text = re.sub('É', 'e', text)\n",
    "    text = re.sub('Í', 'i', text)\n",
    "    text = re.sub('Ó', 'o', text)\n",
    "    text = re.sub('Ú', 'u', text)\n",
    "    text = re.sub('ñ', 'n', text)\n",
    "    text = re.sub('Ñ', 'n', text)\n",
    "    text = re.sub('ª', 'a', text)\n",
    "    text = re.sub('°', 'o', text)\n",
    "    text = re.sub('http[s]*://[^ ]+', '', text)\n",
    "    text = re.sub('#|\\(|\\)|-|`|\"', '', text)\n",
    "    text = re.sub(\"[^a-zA-Z ]\", ' ', text)\n",
    "    return text\n",
    "\n",
    "# separar las tokens de un texto\n",
    "def tokenizar(text):\n",
    "    #tokens = text.split(' ')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# aplicar stemming (reduccion a la forma base) de un texto\n",
    "def stemming(token):\n",
    "    sno = nltk.stem.SnowballStemmer('spanish')\n",
    "    new_token = sno.stem(token).encode(\"utf-8\")\n",
    "    return new_token\n",
    "\n",
    "# entregar los tokens, aplicando: normalizacion, tokenizado, stopwords, stemming\n",
    "def words_doc(text):\n",
    "    #new_text = text.translate(None, string.punctuation)\n",
    "    new_text = normalize(text)\n",
    "    tokens = tokenizar(new_text)#[0:-1])\n",
    "    stop = set(stopwords.words('spanish'))\n",
    "    filtradas = [i for i in tokens if i not in stop]\n",
    "    tokens_text = []\n",
    "    for token in filtradas:\n",
    "        new_token = stemming(token)\n",
    "        tokens_text.append(new_token)\n",
    "    return tokens_text\n",
    "\n",
    "# entrega el vector asociado a un set de palabras (tokens_doc) en una coleccion (dictionary)\n",
    "def vectorization(tokens_doc, dictionary, N):\n",
    "    nwords = len(dictionary)\n",
    "    vector = {}\n",
    "    vector_final = []\n",
    "    for k in range(nwords):\n",
    "        vector[k] = 0\n",
    "    for token in tokens_doc: # cada palabra en este documento\n",
    "        freq     = tokens_doc.count(token)\n",
    "        if token in dictionary.keys():\n",
    "            freq_all = dictionary.get(token)\n",
    "            tf_idf = freq * math.log(N / (1.0 * freq_all))\n",
    "            k = 0\n",
    "            for token2 in dictionary: # cada palabra del diccionario\n",
    "                if token == token2:\n",
    "                    vector[k] = round(tf_idf, 2)\n",
    "                k += 1\n",
    "        else:\n",
    "            tf_idf = 0\n",
    "    for item in vector:\n",
    "        vector_final.append(vector.get(item))\n",
    "    return vector_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets negativos: 230 | tweets neutros: 225 | tweets positivos: 229\n",
      "|dictionary|: 2640\n",
      "|train|: 548, |test|: 136\n",
      "SCORE: 0.66\n",
      "['NEGATIVO' 'NEUTRO' 'POSITIVO']\n",
      "train ok.\n",
      "query: felipe kast\n",
      " . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "NEGATIVO: 26.0%\n",
      "NEUTRO: 8.0%\n",
      "POSITIVO: 49.0%\n"
     ]
    }
   ],
   "source": [
    "docs          = []\n",
    "classes       = []\n",
    "vectors       = []\n",
    "vectors_train = []\n",
    "vectors_test  = []\n",
    "classes_train = []\n",
    "classes_test  = []\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "# lectura de los archivos de textos/documentos\n",
    "f_neg = open(TWEETS_NEG, \"r\")\n",
    "f_neu = open(TWEETS_NEU, \"r\")\n",
    "f_pos = open(TWEETS_POS, \"r\")\n",
    "k = 0\n",
    "while True:\n",
    "    line = f_neg.readline()\n",
    "    if not line: break\n",
    "    list = line.split('|')\n",
    "    docs.append(list[0])\n",
    "    classes.append(list[1][0:-1])\n",
    "    k += 1\n",
    "print \"tweets negativos: \" + str(k),\n",
    "k = 0\n",
    "while True:\n",
    "    line = f_neu.readline()\n",
    "    if not line: break\n",
    "    list = line.split('|')\n",
    "    docs.append(list[0])\n",
    "    classes.append(list[1][0:-1])\n",
    "    k += 1\n",
    "print \"| tweets neutros: \" + str(k),\n",
    "k = 0\n",
    "while True:\n",
    "    line = f_pos.readline()\n",
    "    if not line: break\n",
    "    list = line.split('|')\n",
    "    docs.append(list[0])\n",
    "    classes.append(list[1][0:-1])\n",
    "    k += 1\n",
    "print \"| tweets positivos: \" + str(k)\n",
    "\n",
    "# extraccion de tokens de la coleccion\n",
    "dictionary = {}\n",
    "tokens_docs = []\n",
    "for doc in docs:\n",
    "    tokens_doc = words_doc(normalize(doc))\n",
    "    for token in tokens_doc:\n",
    "        if dictionary.get(token) != None:\n",
    "            dictionary[token] += 1\n",
    "        else:\n",
    "            dictionary[token] = 1\n",
    "    tokens_docs.append(tokens_doc)\n",
    "\n",
    "# guardar diccionario\n",
    "file = open('dictionary.csv', 'w')\n",
    "for token in sorted(dictionary, key=dictionary.get, reverse=True):\n",
    "    file.write(token + ',' + str(dictionary.get(token)) + '\\n')\n",
    "\n",
    "# conformacion de la dimensionalidad de la coleccion\n",
    "N = len(docs)\n",
    "kdoc = 0\n",
    "for tokens_doc in tokens_docs: # cada documento [sus tokens]\n",
    "    vector = vectorization(tokens_doc, dictionary, N)\n",
    "    vectors.append(vector)\n",
    "    kdoc += 1\n",
    "print \"|dictionary|: \" + str(len(dictionary))\n",
    "\n",
    "# separacion de conjuntos de train y test\n",
    "percentage = 0.8\n",
    "for k in range(len(vectors)):\n",
    "    if random.random() < percentage:\n",
    "        vectors_train.append(vectors[k])\n",
    "        classes_train.append(classes[k])\n",
    "    else:\n",
    "        vectors_test.append(vectors[k])\n",
    "        classes_test.append(classes[k])\n",
    "print \"|train|: \"+ str(len(vectors_train))+ \", |test|: \"+ str(len(vectors_test))\n",
    "\n",
    "# preprocesar: convertir clases a numeros\n",
    "label_e = preprocessing.LabelEncoder()\n",
    "label_e.fit(classes)\n",
    "classes_encoded = label_e.transform(classes)\n",
    "classes_train_encoded = label_e.transform(classes_train)\n",
    "classes_test_encoded  = label_e.transform(classes_test)\n",
    "\n",
    "# preparacion modelo clasificador\n",
    "#classifier = KNeighborsClassifier(3)\n",
    "#classifier = SVC(kernel=\"linear\", C=0.025)\n",
    "classifier = LogisticRegression(\n",
    "    solver='lbfgs', max_iter=1000, multi_class='multinomial')\n",
    "classifier.fit(vectors_train, classes_train_encoded)\n",
    "\n",
    "print \"SCORE: \" + str(round(classifier.score(vectors_test, classes_test_encoded), 2))\n",
    "print label_e.inverse_transform([0, 1, 2])\n",
    "print 'train ok.'\n",
    "\n",
    "# iteracion: prediccion de textos\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "maxtweets = 100\n",
    "#while True:\n",
    "print \"query: \",\n",
    "query = raw_input()\n",
    "if len(query) > 0:\n",
    "    query += ' -RT'\n",
    "    search = api.search(q=query, count=maxtweets)\n",
    "    count_neg = 0\n",
    "    count_neu = 0\n",
    "    count_pos = 0\n",
    "    for tweet in search:\n",
    "        tokens = words_doc(tweet.text)\n",
    "        vector = vectorization(tokens, dictionary, N)\n",
    "        this_class = label_e.inverse_transform(\n",
    "            classifier.predict(np.array(vector).reshape(1, -1)))\n",
    "        #print this_class + \" : \" + tweet.text.encode(\"utf-8\").replace('\\n', '')\n",
    "        print \".\",\n",
    "        if this_class == 'NEGATIVO':\n",
    "            count_neg += 1\n",
    "        if this_class == 'NEUTRO':\n",
    "            count_neu += 1\n",
    "        if this_class == 'POSITIVO':\n",
    "            count_pos += 1\n",
    "\n",
    "    print\n",
    "    print \"NEGATIVO: \" + str(100 * count_neg / (maxtweets * 1.0)) + \"%\"\n",
    "    print \"NEUTRO: \"   + str(100 * count_neu / (maxtweets * 1.0)) + \"%\"\n",
    "    print \"POSITIVO: \" + str(100 * count_pos / (maxtweets * 1.0)) + \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
